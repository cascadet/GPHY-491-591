{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24c7299-43bb-4206-9d5e-13dbac4b1770",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Session 8-1: Multiprocessing, Dask and Xarray\n",
    "\n",
    "![ntl](./assets/parallel.png)\n",
    "\n",
    "Recall back to the beginning of the course when we discussed that modern computers have multiple central processing units (CPUs), or cores. Opperating systems, like Windows, are optimized to efficently take advantage of the CPUs on a given system, with hidden subroutines that keep track of CPU loading and RAM allocation. As budding data scientists, we will not go into the details of how a modern computer functions. But we will cover the basics of parallel processing so that you can take advantage of all the CPUs on your system. \n",
    "\n",
    "Python has two relatively new packages - [`dask and xarray`](https://docs.xarray.dev/en/stable/user-guide/dask.html) - that enable to processing of gridded datasets in parallel with ease. But to understand how these packages work together, we first need to spend a moment looking at native [`python multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) module.  \n",
    "\n",
    "A few things to remember. When you launch a **Earth Sciences JupyterLab Classroom** instance of Jupyter on Tempest, the system allocates 8 CPUS and 32GB of RAM for you to use (FYI - Tempest is one of the largest university clusters in the USA, with Tempest currently has 12,264 logical CPU cores, 55.9TB of ECC memory). Thus, if you want to maximize the power of your Jupyter instance, you can have up to 8 CPUs working with 4 GB of RAM (32/8 = 4). \n",
    "\n",
    "But when tell a CPU to preform a task and it gets allocated a pool of memory, the data being processed on that CPU cannot access the data being processed on another CPU due Python's [Global Interpreter Lock](https://realpython.com/python-gil/). You don't need to understand the GIL in detail for this class. What you do need to know, is that to leverage all the CPUs you need to chunk your data into logical blocks that each CPU can workon without having to talk to the other CPUs. \n",
    "\n",
    "This might make you a bit confused. It made me very confused when I started teaching this to myself back in graduate school. The easiest way to learn, as we know is to play some code. So let's do that!\n",
    "\n",
    "<p style=\"height:1pt\"> </p>\n",
    "\n",
    "<div class=\"boxhead2\">\n",
    "    Session Topics\n",
    "</div>\n",
    "\n",
    "<div class=\"boxtext2\">\n",
    "<ul class=\"a\">\n",
    "    <li> üìå Introduction to <span class=\"codeb\">multiprocessing</span> </li>\n",
    "    <ul class=\"b\">\n",
    "        <li> Census API </li>\n",
    "        <li> Merging with shapefiles </li>\n",
    "        <li> Plotting Data </li>\n",
    "        <li> Area Aggregation </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border-top: 0.2px solid gray; margin-top: 12pt; margin-bottom: 0pt\"></hr>\n",
    "\n",
    "### Instructions\n",
    "We will work through this notebook together. To run a cell, click on the cell and press \"Shift\" + \"Enter\" or click the \"Run\" button in the toolbar at the top. \n",
    "\n",
    "<p style=\"color:#408000; font-weight: bold\"> üêç &nbsp; &nbsp; This symbol designates an important note about Python structure, syntax, or another quirk.  </p>\n",
    "\n",
    "<p style=\"color:#008C96; font-weight: bold\"> ‚ñ∂Ô∏è &nbsp; &nbsp; This symbol designates a cell with code to be run.  </p>\n",
    "\n",
    "<p style=\"color:#008C96; font-weight: bold\"> ‚úèÔ∏è &nbsp; &nbsp; This symbol designates a partially coded cell with an example.  </p>\n",
    "\n",
    "<hr style=\"border-top: 1px solid gray; margin-top: 24px; margin-bottom: 1px\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080644f-bb8a-45ad-9a7f-44ec38d0c1de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# multiprocessing\n",
    "\n",
    "The [`python multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) module is one of the things that make Python such an awesome tool and why Python is among the most popular languages for data science. This is because it make scaling code across CPUs quite easy. We are not going to go deep into the package in this class. But I want to introdroduce you to it, because understanting [`python multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) will make learning [`dask and xarray`](https://docs.xarray.dev/en/stable/user-guide/dask.html) much easier.\n",
    "\n",
    "To get started, let's start with a simple example. Let's say you have global, daily maximum heat index data for 2016 that you want to convert from celcius to farenheit and then find the global average temperature for each day. In other words, you have 366 (2016 is a leap year) raster files on which you need to preform simple multiplication. These files are pretty large (76mb) and about 9 GB in total data. \n",
    "\n",
    "As you will see below, you can do this with a `for` loop. But it will be faster to do this in parallel. Let's take a look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0280b6-e2e4-4b59-89bf-c2a8bdfc492e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3505b5-deb0-4968-9efb-4e025c84005c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob  \n",
    "import sys\n",
    "from multiprocessing import Pool \n",
    "import time \n",
    "import numpy as np\n",
    "import rasterio\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f4db96-8c7e-4876-baed-b8c875bfc66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the path to your files in a list files\n",
    "path = os.path.join('/home/group/earthsciences/gphy591/github/GPHY-491-591/materials/Day8/data/2016/')\n",
    "fns = sorted(glob.glob(path + '*.tif'))\n",
    "\n",
    "# print first five files to make sure they are in order\n",
    "fns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a110c-0838-4bc5-a4f5-ec69ed8645d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First, let's take a look at the data\n",
    "Open one raster and plot it, then look at the meta data to see what the NaN values are so we don't mess those up when we convert ¬∞C to ¬∞F. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647a135-3356-4e21-8dd0-4151decf9914",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4d1bd-d78c-475f-998e-919807942c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the data for July 1, 2016\n",
    "arr = rasterio.open(fns[182]).read(1)\n",
    "plt.imshow(arr, vmin = -32)\n",
    "plt.colorbar(shrink = 0.4)\n",
    "plt.title('Maximum Heat Index in ¬∞C for July 1, 2016');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468d6fd-6d56-467d-9e9a-6e7dfb1280aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at the NAN values\n",
    "rasterio.open(fns[182]).meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d680f-a98e-4668-947b-f1d71e514cc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now Let's try a `for` loop. \n",
    "\n",
    "Remember, our NaN values is `-9999` so we want to make sure we mask that out when we calculate our global average HI for each day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04f3d8-37fe-42b5-b39d-1059bbd1168e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21082aed-9a0c-4b8b-b740-f939ec42d2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to convert c to f\n",
    "def C_to_F(Tmax_C):\n",
    "    \"Function converts temp in C to F\"\n",
    "    Tmax_F = (Tmax_C * (9/5)) + 32\n",
    "    \n",
    "    return Tmax_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53282d6a-1498-4021-9152-38fb4f23cb87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clock it\n",
    "start = time.time()\n",
    "\n",
    "for fn in fns:\n",
    "    \n",
    "    # Get the date\n",
    "    date = fn.split('data/2016/')[1].split('.tif')[0]\n",
    "    \n",
    "    # open the array in c\n",
    "    arr_c = rasterio.open(fn).read(1)\n",
    "    \n",
    "    # covert c to f, using np where so we keep the -9999 values\n",
    "    arr_f = np.where(arr_c != -9999, C_to_F(arr_c), -9999) # this says, where arr_c does not equal -9999, covert data from c to f, but everwhere else write -9999\n",
    "    \n",
    "    # get the daily global average maximum heat index \n",
    "    land = arr_f[arr_f != -9999] # drop all ocean -9999 values\n",
    "    avg = land.mean()\n",
    "    \n",
    "    print('On', date, 'the global average heat index was', round(avg, 2), '¬∞F')\n",
    "    \n",
    "end = time.time()\n",
    "print('Time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c359e-333f-4c2d-a6d1-575201f612d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now try it in parallel. \n",
    "Running the code sequentially takes about 33 seconds on one CPU. That's not too bad, but what if you need to do run this calculation 1,000 times? That will add up, and, if you make a mistake, and need to re-run the code, it will set you back even further. This is where parallel processing is super useful. \n",
    "\n",
    "Let's see how fast this goes when we feed our list of files to all 8 CPUS. To do this we have to:\n",
    "\n",
    "1. Create a function to pass to multiprocessing.\n",
    "2. Create a pool of works (e.g. cpus)\n",
    "3. Pass our function and list for our works to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3d775-e8e1-498c-9686-58276d2096a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb561dd0-25c2-4121-9a1d-df04c466f18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_fast(fn):\n",
    "    \n",
    "    \"Function takes a global heat index raster, coverts the data from C to F, and caluclates the average of the value for that raster, assuming NaN = -9999\" \n",
    "    \n",
    "    # you can print your work id\n",
    "    # print(multiprocessing.current_process())  # for now, we'll leave this commented out\n",
    "    \n",
    "    # Get the date\n",
    "    date = fn.split('data/2016/')[1].split('.tif')[0]\n",
    "    \n",
    "    # open the array in c\n",
    "    arr_c = rasterio.open(fn).read(1)\n",
    "\n",
    "    # covert c to f, using np where so we keep the -9999 values\n",
    "    arr_c = np.where(arr_c != -9999, C_to_F(arr_c), -9999) # this says, where arr_c does not equal -9999, covert data from c to f, but everwhere else write -9999\n",
    "    \n",
    "    # get the daily global average maximum heat index \n",
    "    land_c = arr_c[arr_c != -9999] # drop all ocean -9999 values\n",
    "    avg_c = land_c.mean()\n",
    "    \n",
    "    # print('On', date, 'the global average heat index was', round(avg, 2), '¬∞F \\n')\n",
    "    print(date, avg_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b702f1-76e7-48f3-a5a9-461cd2b3b986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clock it\n",
    "start = time.time()\n",
    "\n",
    "# set up your pool of works, in this case we have 8 \n",
    "n_cpus = 8 \n",
    "pool = Pool(processes = n_cpus)\n",
    "\n",
    "# map the function and the arguments to the pool of works, in this case avg_fast and the list of files\n",
    "pool.map(avg_fast, fns)\n",
    "\n",
    "# shut down your pool of workers\n",
    "pool.close()\n",
    "\n",
    "end = time.time()\n",
    "print('Time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c25f0-7a53-4243-9705-2869e4152272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Some things to think about. \n",
    "\n",
    "**Wow!** That was quite a bit faster than one CPU working on the data sequentially.\n",
    "\n",
    "Notice that the `print` statements from each worker are not printed sequentially. This is because each CPU is working independently and the output is spit out when then the CPU is done. The GIL makes sure that the output from each worker doens't get mixed up, but because this is a relatively small computation for a CPU, there is not enough time delay between tasks for Python to print each output from a worker. \n",
    "\n",
    "But if you have each CPU cruch a lot more data, then each worker will will be slowed down enough that the outputs get printed sequentially. Below is a an example where we convert the data from c to f 40 times, to show you what a bigger job looks like. But we are only going to run this on 4 cpus and the first 30 files (e.g. `fns[:30`) as an  example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211f12b-93ff-4076-8c97-dccd68e0737f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_slow(fn):\n",
    "    \n",
    "    \"Function takes a global heat index raster, coverts the data from C to F, and caluclates the average of the value for that raster, assuming NaN = -9999\" \n",
    "    \n",
    "    # you can print your work id\n",
    "    # print(multiprocessing.current_process())  # for now, we'll leave this commented out\n",
    "    \n",
    "    # Get the date\n",
    "    date = fn.split('data/2016/')[1].split('.tif')[0]\n",
    "\n",
    "\n",
    "    # Conver the data from c to f 10 times to slow everything down   \n",
    "    for i in list(range(40)):\n",
    "        \n",
    "        # open the array in c\n",
    "        arr_c = rasterio.open(fn).read(1)\n",
    "        \n",
    "        # covert c to f, using np where so we keep the -9999 values\n",
    "        arr_f = np.where(arr_c != -9999, C_to_F(arr_c), -9999) # this says, where arr_c does not equal -9999, covert data from c to f, but everwhere else write -9999\n",
    "    \n",
    "    # get the daily global average maximum heat index \n",
    "    land_f = arr_f[arr_f != -9999] # drop all ocean -9999 values\n",
    "    avg_f = land_f.mean()\n",
    "    \n",
    "    # print('On', date, 'the global average heat index was', round(avg, 2), '¬∞F \\n')\n",
    "    print(multiprocessing.current_process(), date, avg_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32619da9-a8d2-4651-8e9c-d29c6bf91471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clock it\n",
    "start = time.time()\n",
    "\n",
    "# set up your pool of works, in this case we have 4 \n",
    "n_cpus = 4 \n",
    "pool = Pool(processes = n_cpus)\n",
    "\n",
    "# map the function and the arguments to the pool of works, in this case avg_fast and the list of files\n",
    "pool.map(avg_slow, fns[:30])\n",
    "\n",
    "# shut down your pool of workers\n",
    "pool.close()\n",
    "\n",
    "end = time.time()\n",
    "print('Time:', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea031ff-e363-4de2-89c8-59464c861a84",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"border-top: 1px solid gray; margin-top: 24px; margin-bottom: 1px\"></hr>\n",
    "\n",
    "# Dask & Xarray \n",
    "\n",
    "[`python multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) is a great package to use if you want to crunch a bunch of files in parallel. You can even `chunk` datasets and feed lists of chunks to your `Pool` to process data in parallel. But things get more complex when you want the outputs of each CPU's tasks to talk with each other. \n",
    "\n",
    "For example, let's say we want to calculate the daily average heat index for each _pixel_ in the dataset. This would require a 3-d array (or cube), where our _x_ and _y_ axes are _longitude_ and _latitude_, respecitively, and our _z_ axis is _time_. Is is possible to open each raster and stack them into a 3-d data cube. But remember, that would require a huge amount of memory since the total data is about 9 GB of data. When we open those rasters, they will balloon because `GeoTiff` files are compressed to some degree too.\n",
    "\n",
    "![ntl](./assets/data.png)\n",
    "\n",
    "Welcome to [`Dask and Xarray`](https://docs.xarray.dev/en/stable/user-guide/dask.html)! They will make your life a lot easier. You complete an entire free online tutorial if you want to go deep on `Dask and Xarray`, [An Introduction to Earth and Environmental Data Science](https://earth-env-data-science.github.io/intro.html). Here, we are just going to touch on the basics of both packages, but I **highly** recommend you read about `Xarray` data structures [here](https://docs.xarray.dev/en/stable/user-guide/data-structures.html) because the terminology can be a bit confusing.\n",
    "\n",
    "![ntl](./assets/xarray.png)\n",
    "\n",
    "Basically, `Xarray` allows you to create labeled n-dimentional numpy arrays. So you can label your datasets (temperature, precipitation, etc.) and your dimentions (time, lat/long, etc.) to easily subset the data to run analysis. For example, you could say, what is the average heat index in Bozeman based on Lat/Long with just a few lines of code. **Note** We will use the package [`rioxarray`](https://corteva.github.io/rioxarray/html/rioxarray.html) too to load in the GeoTiff files. More on this later. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63011d76-8693-4f06-80d9-404764b0a7cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a9dbc-8a2f-4731-b3fd-b62d8749a3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencies \n",
    "import xarray as xr\n",
    "import dask\n",
    "import rioxarray as rio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee0ab3-9414-413f-b8b9-55e4dc04c706",
   "metadata": {},
   "source": [
    "### A Toy Dataset\n",
    "Let's start by making two toy data xarray data arrays and turn them into an xarray dataset with temperature and precipitation. They will be 100 by 100 by 30 arrays, representing 100 by 100 lat/long and 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26269f-7800-4c75-b999-9f4d94682832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make an empty temperature array\n",
    "temp = np.random.randint(20, high=100, size=(100,100,30), dtype=int)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437590b-fba2-44a8-8f87-160d88169a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make an empty precip array\n",
    "precip = np.random.randint(0, high= 10, size=(100,100,30), dtype=int)\n",
    "precip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067a4c3-20b7-4e1d-bcad-5efd0d4965a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a 30 day time stamp\n",
    "time = pd.date_range(\"2016-01-01\", periods=30)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ddb10-16a2-427c-9151-502375979666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c2e39-c65b-4bfc-bc4c-0cfdcef100d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make x and y index values\n",
    "x = list(range(1,100+1))\n",
    "y = list(range(1,100+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd10db-8cb2-4938-8fc9-fdbab34613d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn temp into xarray data array\n",
    "temp_da = xr.DataArray(data = temp, # data\n",
    "                       dims = ['x', 'y', 'time'], # dim labels as a list\n",
    "                       coords = {'x' : x, 'y' : y, 'time' : time}, # coords data as a dict\n",
    "                       name = 'temp' # name the da\n",
    "                      )\n",
    "temp_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0b38e-0999-4a35-8ce6-fba2819bc5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn precip into xarray data array\n",
    "precip_da = xr.DataArray(data = precip, # data\n",
    "                       dims = ['x', 'y', 'time'], # dim labels as a list\n",
    "                       coords = {'x' : x, 'y' : y, 'time' : time}, # coords data as a dict\n",
    "                       name = 'precip' # name the da\n",
    "                      )\n",
    "precip_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd862162-2f72-42cb-89a2-a362d98c1bd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now combine temp and precip into a dataset\n",
    "ds = xr.merge([temp_da, precip_da])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53220d0-0e07-47b4-9964-18618d78e111",
   "metadata": {},
   "source": [
    "#### A bit about Xarray Datasets\n",
    "\n",
    "Xarray Datasets are nice because you can easily stack several data arrays and run analysis on the data. Like numpy arrays, they must be the same size. But unlike numpy arrays, you have labels, so it is easy to subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd3207-3d2a-4485-8fb2-c18ce1fd3152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# access the temp data\n",
    "ds.temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e102b9-2807-4ce5-9544-5b4daa735ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# access the precip data\n",
    "ds.precip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30bff4-001f-4d63-a3aa-abcf3a9f4c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate correlation between precip and temperature over time\n",
    "corr = xr.corr(ds.temp, ds.precip, dim= 'time')\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2ec3b-4307-4d2a-9ba9-46b1aa31362e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's look at some real data with Dask\n",
    "\n",
    "Using `Xarray` and `Dask` together is really powerful. Be sure to read this [overview](https://docs.xarray.dev/en/stable/user-guide/dask.html) on your own time. But the short of it is that `Dask` allows you to create `chunks` of large datasets that are small enough to load into memory, without loading all the data into memory. When used with `xarray`, you can set up a instructions (e.g. computations) to run on `Xarray` object that Python will complete in parallel without actually loading the dataset into memory. Confused? I was too. The easiest way to learn this, is to actually do it. \n",
    "\n",
    "Let's look at an example with our heat index data from 2016 that is nearly 9 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7d1e9-e90b-437e-b03d-6b176b9d5b87",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806c9ad-e466-40d2-a7ae-d15335f4b0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the path to your files in a list files\n",
    "path = os.path.join('/home/group/earthsciences/gphy591/github/GPHY-491-591/materials/Day8/data/2016/')\n",
    "fns = sorted(glob.glob(path + '*.tif'))\n",
    "\n",
    "# print first five files to make sure they are in order\n",
    "fns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2ab91-df24-4033-a9ad-5920425378bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load a 'view' of all 366 files into a Xarray DataSet\n",
    "\n",
    "The code below will not load the files into memory, but it will create a DataSet on which we can look at what the data would be if it were loaded into memory, set up a set of instructions, and then tell `Dask` to run the instructions in parallel.\n",
    "\n",
    "Like python `multiprocessing`, you first have to tell `Dask` to tee up 8 CPUS. If you run this on your on machine, `Dask` creates a URL where you can watch your workers work. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20125ec9-2412-4c94-9fa6-2cf2a5ab8e0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"run\">\n",
    "    ‚ñ∂Ô∏è <b> Run the cells below. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31ec9f-4580-4548-ae97-8fff695b9f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencies \n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33035f01-90d9-47e2-b179-b03124197f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create and connect to a dash cluster + get link to watch progress\n",
    "client = Client(n_workers = 8)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a024558-9fbd-4bbf-a78a-b76269e57432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now 'load' the GeoTif Files\n",
    "da = xr.concat([rio.open_rasterio(f, chunks = 'auto') for f in fns], dim='band') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e0c3c-b8d5-4473-8c76-fbb9798b26e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take a look at your DataArray\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9e4eb-5a10-44c7-9be1-8e9984ed8a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How big is the da object?\n",
    "sys.getsizeof(da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e51e3-f003-41cb-9ad5-271f67f1977b",
   "metadata": {},
   "source": [
    "As you see, the da object is **REALLY** small. That is because the data is not actually in memory. `Dask` has chunked the data for us so it will run in parallel using `auto` argument, where `dask` figures out the optimal size of the chunk give our CPUs and available memory.\n",
    "\n",
    "### Updating Data\n",
    "We can update information about our data array, like renaming `band` to time and we can add a `Pandas` time series to the `time` dimention as coordinates, again without actually loading the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26015bcf-3b67-400a-975f-54244427a532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a pandas time daily time series for 2016\n",
    "time = time = pd.date_range(\"2016-01-01\", periods=366)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da04456-3905-4cd1-82a7-d156b667f984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rename time dim\n",
    "da = da.rename({'band' : 'time'})\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c4b2e-bda7-44d0-bfa0-d1feb1be6c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Revalue the time' coord\n",
    "da.coords['time'] = time\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec82d65-2b44-405f-add2-bd88c7fe03d8",
   "metadata": {},
   "source": [
    "### Subsetting data\n",
    "\n",
    "There are tons of ways to slice Xarray Data Arrays. Here are two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa6092-8988-44a4-a070-0bce9fba2491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select by month (e.g. January)\n",
    "jan = da.where(da['time.month'] == 1, drop = True)\n",
    "jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157d3fa-dac7-42e0-8f74-0a0f429990c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select by set by col / row\n",
    "subset = da[:, 1000:1500, 3000:4000] # subset for west africa\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf09764-099b-40bf-8320-bd1794d6e939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Slice by lat / long\n",
    "subset = da.sel(x = slice(-30,19), y = slice(20, -5))\n",
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fc8de-017e-4b7d-bcbb-6f35e49878cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Average Heat Index\n",
    "Now, let's calculate the daily and monthly maximum heat index for 2016 with dask! Again, nothing is actually being computed, until you add `.compute()` to the data array. Check out the code below to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba5cef-b4cd-4059-92f4-94b1ab23c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate the mean daily max heat index along the time dim\n",
    "daily_avg = da.mean(dim = 'time')\n",
    "daily_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a890f-820e-42fd-93d5-c949e1e63093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now run the code again with .compute() added\n",
    "daily_avg = da.mean(dim = 'time').compute()\n",
    "daily_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd597e-d4f1-4917-bcd5-4f9935ca8e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot it \n",
    "arr = daily_avg.data\n",
    "plt.imshow(arr, vmin = -32)\n",
    "plt.colorbar(shrink = 0.3)\n",
    "plt.title('Average Daily Maximum Heat Index for 2016');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9092939-f6ca-41d4-9d6d-3521fdc06dc4",
   "metadata": {},
   "source": [
    "### Save your the daily average heat index for 2016 as a tif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f787b-7414-4b7b-bfc6-e8f612da1c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the meta data\n",
    "meta = rasterio.open(fns[0]).meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9dee0b-44a1-4327-97eb-5281fda11ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a file name\n",
    "fn_out = os.path.join('./himax_2016_avg.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac11af-1997-48f1-83d1-2a97a0ea47f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save it \n",
    "with rasterio.open(fn_out, \"w\", **meta) as dest:\n",
    "    dest.write(arr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b6360-5a80-4d47-ac24-9ee9b1566650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check it\n",
    "plt.imshow(rasterio.open(fn_out).read(1), vmin = -32);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af24a03-8a67-444f-8611-a9757ad83c3a",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
